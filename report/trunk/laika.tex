\section{Laika}
\label{sec:laika}

\begin{figure}[h]
\centering
\includegraphics[width=4.8in,clip,trim=0cm 7cm 0 0]{figures/laika_diagram.pdf}
\caption{Diagram describing the scheduling algorithm \proc{Laika}
detailed in \figref{laika_code}.  The diagram consists of $2^b$-vertex
'chunks', where $b$ is a configuration parameter, 
each of which is processed serially.  Due to the locality resulting
from the Hilbert priority function, the neighbors of vertices
labeled $u$ and $v$, respectively, predominantly lie in the shaded regions
surrounding them.  Chunks are processed serially, so it is not necessary
to keep track of predecessors and successors that lie within a single
chunk.  As a result, vertex $u$ merely executes its update function 
and incurs no overhead for updating the counters of its neighbors, as
with \proc{JonesPlassmann}.  The vertex $v$ illustrates a common 
phenomenon, where vertices near the beginning of a chunk 
(i.e. chunk 3) have successors toward the end of the previous 
chunk (i.e. chunk 2), a \emph{backward dependency}.}
\label{fig:laika_diagram}
\end{figure}

In this section we describe a new scheduling algorithm
called \proc{Laika} which takes advantage of the reordering
of the vertices by the Hilbert priority function.  It is
a priority-dag scheduling algorithm in that each vertex has
a counter which is decremented for each predecessor that is
processed, however many of the (atomic) decrements are removed
by design in \proc{Laika} to reduce overhead.  A
diagram emphasizing aspects of \proc{Laika} can be
found in \figref{laika_diagram} and pseudocode can be
found in \figref{laika_code}.  The algorithm
works by breaking up the vertices into contiguous chunks of
size $2^b$, where $b$ is a configuration parameter, and making
progress on each chunk in rounds.  In each round, the vertices
within a chunk are processed serially and each chunk is processed
in parallel.  If at some point during the course of processing
a chunk a vertex $v$ is encountered which is not yet ready to be
processed (i.e. $\attrib{v}{\id{counter}} > 0$), then the current
position in the chunk is recorded and progress is suspended until
the next round.  Due to the cache-locality property achieved
by reordering, much progress is able to be made on each chunk,
necessitating relatively few rounds. 


\begin{SCfigure}
\centering
%\vspace{-2ex}
  %\begin{tabular}{@{}ll@{}}
  \begin{minipage}[t]{.6\linewidth}
      \begin{codebox*}
        \Procname{$\proc{Laika}(G,b)$}
        \li let $G=(V,E,\rho)$
        % \li \Parfor $i \in [0,\card{V})$ \Do
        % \li   $\attrib{v_i}{\id{chunk}} \gets \floor{i/2^b}$
        % \li   $\attrib{v_i}{\id{priority}} \gets i \pmod {2^b}$
        %     \End
        \li \Parfor $v \in V$ \Do
        \li $S = \set{w \in \adj{v} : \attrib{w}{\id{chunk}} \neq \attrib{v}{\id{chunk}}}$
        \li   $\preds{v} = \set{w\in S : \rho(w) < \rho(v)}$
            \lilabel{jp_preds}
        \li   $\succs{v} = \set{w\in S : \rho(w) > \rho(v)}$
            \lilabel{jp_succs}
        \li $\counter{v} \gets \card{\preds{v}}$
            \End 
        \li \Parfor $i \in [0,\card{V}/2^b)$ \Do
        \li   $\id{chunkPointer}[i] \gets i2^b$
            \End
        \li $\id{done} \gets \const{False}$
        \li \While $\id{done} \isequal \const{False}$ \Do
        \li   $\id{done} \gets \const{True}$
        \li   \Parfor $i \in [0,\card{V}/2^b)$ \Do
        \li     $p \gets \id{chunkPointer}[i]$
        \li     \While $p < (i+1)2^b$ \Do
        \li       \If $\attrib{v_p}{\id{counter}} \isequal 0$ \Do
        \li         $\proc{Update}(v_p)$
        \li         \Parfor $w \in \succs{v_p}$ \Do
        \li           $\attrib{w}{\id{counter}} = \attrib{w}{\id{counter}} - 1$
                    \End
        \li         $p \gets p + 1$
        \li       \Else
        \li         $\id{done} \gets \const{False}$
        \li         $\const{Break}$
                  \End
                \End
        \li   $\id{chunkPointer}[i] \gets p$
              \End
            \End  
      \end{codebox*}
    \end{minipage}
    %&  
    \caption{The Laika scheduling algorithm.  The main \textbf{while}
    loop spins making progress on all chunks in parallel 
    until all chunks have been completely processed.  For
    each iteration, each chunk is processed serially until it is
    either done or encounters a vertex $v$ which is not yet eligible to
    be processed (i.e. $\attrib{v}{\id{counter}} > 0$).  The priority
    function $\rho(v)$ returns the pair 
    $\langle \attrib{v}{\id{priority}},\attrib{v}{\id{chunk}} \rangle$
    which is comparable lexicographically.  The user-supplied
    function \proc{Update} is called for each vertex once all of its
    predecessors have been processed.  The vertex state for the $i$th
    vertex $v_i$, 
    $\attrib{v_i}{\id{chunk}} \gets \floor{i/2^b}$ and 
    $\attrib{v_i}{\id{priority}} \gets i \pmod {2^b}$ is defined
    at graph creation.}
\label{fig:laika_code}
\end{SCfigure}






% Once we have a good vertex partitioning and a strategy for distributed execution, throughput depends largely on single-machine performance. We explore a number of schemes to this end.


% \subsection{Graph Representation in Memory}
% We represent graphs in memory on a single machine as follows. We have an array of vertices and an array of edges. Each vertex contains data and a pointer into the edge array indicating the start of its list of edges. Adjacent vertices have adjacent edge lists. Each edge is simply a pointer into the vertex array. This organization is shown in Figure 3.

% \subsection{Scheduling for Parallel Execution}
% There are two major strategies for scheduling vertices for parallel execution that preserve the appearance of a global ordering across updates and avoid data races. The first is coloring \cite{KalerHaSc14}. If we color a graph so that no two neighboring vertices have the same color, we can safely execute the updates for vertices of the same color in fully in parallel. The reason is that if any vertex's data is being written by some thread, it cannot be read in parallel by another thread because this would require a neighbor of this vertex to be updating concurrently, which is impossible. With the coloring strategy, we sort the vertex array (and the corresponding edge lists) by color and step sequentially through the colors, updating the vertices of each color in a parallel loop.

% The other strategy is priority DAG scheduling \cite{JonesPl93}. This involves assigning each vertex a distinct priority, so that we can form a DAG from our graph by adding a direction to each edge such that the source is the endpoint vertex with higher priority. We assign each vertex a counter equal to the number of predcessors it has. We can start by executing all vertices with no predecessors in parallel. Once a vertex is complete, we atomically decrement the counter of each of its successors. If a vertex's counter becomes zero, we can spawn the update of this vertex as another parallel strand of execution. We thus attain fairly high parallelism at the cost of using atomics, which can involve expensive memory barriers on modern hardware. Once again, there can be no data races because two neighboring vertices cannot execute concurrently since one must be the predecessor of the other.

% \subsection{Achieving Cache Locality}
% If we update the vertices in the vertex array sequentially, as we do with coloring-based scheduling, we get good cache locality (cached lines are processed completely after being fetched) on our accesses to the vertices being updated and to their corresponding entries in the edge array, which is also processed sequentially. Cache locality here includes data cache and TLB locality, since pages in the vertex and edge arrays corresponding to vertices being updated are processed completely after their first access. TLB misses have been shown to be a significant factor in the runtimes of data-intensive computations and are an important consideration for us.

% While we get good locality on edge and vertex array accesses for vertices being updated, we get poor locality for accesses to the vertex array to fetch these vertices' neighbors. These accesses are essentially random unless we have sorted the vertex array in some locality-improving manner. Ideally we could store vertices' neighbors close to them in the vertex array. This would confer two benefits. First, TLB misses would fall since neighbors of a vertex will in most cases be stored in the same page as the vertex. Secondly, if a vertex being updated pulled neighbors that were yet to be updated into cache, those neighbors would be updated before they left cache, reducing cache misses.

% These considerations suggest that ordering vertices in the vertex array by breadth-first search (BFS) level would be helpful. A vertex at BFS level $n$ can only have neighbors at BFS levels $n-1$ and $n+1$; if there was a neighbor at a smaller level, the vertex would have smaller level than $n$, and there can be no neighbors at levels greater than $n+1$ if the vertex is at $n$. Thus, if BFS levels are fairly small, ordering the vertices by BFS level would result in nearby neighbor accesses as desired. BFS levels are generally bounded in size in mesh graphs; they grow at first, but once the largest cross-section of the mesh is reached, successive levels should have similar size. The problem with ordering the entire vertex array by BFS level is that there are no longer defined sequential regions over which parallel update loops can be run -- since any two adjacent vertices could be neighbors -- so parallelism is lost. We implement and evaluate a hybrid coloring-BFS approach that restores some parallelism while preserving our locality wins: within each BFS level, we sort by color, so that within each BFS level we can update the vertices of each color in parallel. The BFS levels are executed sequentially. This scheme brings up an important point about the locality-parallelism design space -- after a point, increasing parallelism is not necessarily important. If there is enough parallelism to saturate the cores of the available machines, locality is likely the parameter worth optimizing.

% % Analyze for priority DAG case
% In the case of priority DAG scheduling, cache behavior is very different. We lose the locality of access to the vertex and edge arrays for vertices being updated that we have in the sequential processing case. But we are not without victories: when we update the last predecessor of some node we immediately afterwards update that node, at which point is hot in cache. So accesses to neighbors of vertices being updated do not have worst-case cache behavior as they do in the sequential processing case.

% For ideal cache behavior, the story is similar to the sequential processing case. We would like for neighborhoods of nearby vertices to be stored fairly contiguously in the vertex array so that accesses to neighbors of vertices being updated tend not to cause TLB misses. We would also like these neighborhoods to be updated completely in some small time window so that fetched neighbors are updated before they leave cache. Achieving the first objective is possible by using a BFS-based ordering or by preserving the Z-number ordering described above for partitioning vertices across machines. We take the second approach because BFS levels may be larger than memory pages and therefore TLB misses are more likely under the BFS-based ordering. Achieving the second objective is much harder with DAG scheduling because the order in which vertices are updated is unclear. However, if we assign each vertex a priority equal to its Z-number, we suspect that if we spawn off the processing of vertices with no predecssors in order of Z-number and if we can assume that earlier spawned routines tend to execute to completion before later spawned routines begin executing (as is the case in the Cilk model of multithreading), then contiguous regions of the physical mesh should be processed nearly to completion in small periods of time. 

% % "small period of time" is not precise...

% %Draw parallel to cache-oblivious?

% \subsection{Considering Parallelism}
% % atomics, barriers in the priority DAG case
% % cache transfers across processors are fast -- so as long as the data is in some cache, we are OK (not for TLB though) ... BUT
% % Cilk -- steals high in the tree

% \subsection{Prefetching}
