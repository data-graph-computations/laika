\relax 
\@writefile{toc}{\contentsline {title}{6.886 Final Project: \unskip \ \ignorespaces  Bigger and Faster Data-graph Computations for Physical Simulations}{1}}
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{Predrag Gruevski, William Hasenplaugh, and James J. Thomas}{1}}
\newlabel{abstract}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\citation{CuttingCa05,DeanGh08}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A mesh graph where lines correspond to edges and intersections of lines correspond to vertices.}}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mesh}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graphs are stored in memory on a single cache-coherent multi-core in a sparse-matrix format. The vertex array contains vertex data and an index into an edge array, which contains vertex IDs of the associated neighbors.}}{2}}
\newlabel{fig:layout}{{2}{2}}
\citation{LowBiGo12}
\citation{KalerHaSc14}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Data-graph Computations}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of how a graph can be partitioned into independent sets of vertices denoted by color, each set of which is able to be executed simultaneously without causing data races. Iterating through the colors serially and executing the corresponding independent sets in parallel is a technique called \emph  {chromatic scheduling}.}}{3}}
\newlabel{fig:chromatic_scheduling}{{3}{3}}
\citation{JonesPl93}
\citation{HasenplaughKaLe14}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An alternative to chromatic scheduling, which yields a deterministic, data race-free output, is dag scheduling. A priority funcrtion $\rho : V \rightarrow \mathbb  {R}$ is used to create a partial order on the vertices, orienting an edge from low to high priority results in a dag. The vertices are processed in dag order: a vertex is not processed until all of its predecessors have been processed.}}{4}}
\newlabel{fig:dag_scheduling}{{4}{4}}
\newlabel{li:jp_preds}{{3}{4}}
\newlabel{li:jp_succs}{{4}{4}}
\newlabel{li:partition)}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The Jones-Plassmann parallel priority-dag scheduling algorithm. \textnormal  {\scshape  JonesPlassmann} uses a recursive helper function \textnormal  {\scshape  JP-Update} to process a vertex using the user-supplied \textnormal  {\scshape  Update} function once all of its predecessors have been updated, recursively calling \textnormal  {\scshape  JP-Update} on any successors who are eligible to be updated. The function \textnormal  {\scshape  Join} decrements its argument and returns the post-decrement value.}}{4}}
\newlabel{fig:jones_plassmann}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Simit}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Graphs generated by the language Simit feature hyperedges, an example of which is in blue on the left. Hyperedges are represented by different \emph  {types} of vertices in the resulting data-graph computation. The square vertices in the figure represent hyperedges and have associated per-hyperedge data.}}{5}}
\newlabel{fig:hyperedge}{{6}{5}}
\citation{Hilbert70}
\citation{Hilbert70}
\citation{Hilbert70}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Graphs generated by the language Simit have tetrahedrons, as depicted on the left above. A tetrahedron is composed of four hyperedges (or \emph  {faces}), an example of which is in blue on the left. Tetrahedra are represented by different \emph  {types} of vertices in the resulting data-graph computation. The diamong vertex on the right represents a tetrahedron and is connected to its four constituent hyperedges.}}{6}}
\newlabel{fig:tetrahedron}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The Hilbert Space-filling Curve}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Three recursion levels of a 2D Hilbert space-filling curve\nobreakspace  {}\cite  {Hilbert70}. }}{6}}
\newlabel{fig:2d_hilbert}{{8}{6}}
\citation{GotsmanLi96}
\citation{MoonJaFa96,TirthapuraSeAl06}
\citation{LowBiGo12}
\citation{KyrolaBlGu12}
\citation{NYT57}
\citation{SinghHoHe93,WarrenSa93,PilkingtonBa96}
\citation{MoonJaFa96}
\citation{SinghHoHe93,WarrenSa93}
\citation{MoonJaFa96}
\citation{LeungPhJo02}
\citation{HarlacherKlRo12}
\citation{TirthapuraSeAl06}
\citation{TirthapuraSeAl06}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The Russian street dog Laika is one of the first and most famous animals to travel through space.}}{7}}
\newlabel{fig:laika}{{9}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Paper organization}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reordering Vertices for Cache Locality}{7}}
\newlabel{sec:reordering}{{2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Theoretical and empirically observed miss rate curves. The red line is generated by the equation $C^{-1/4}$, a consequence of the analysis in Tirthapura et al.\nobreakspace  {}\cite  {TirthapuraSeAl06} of generic recursively-defined space-filling curves. Hilbert curves are known to have better locality in practice. For example, the blue dots are empirically measured from a test graph with 50M vertices described in Section\nobreakspace  {}3.1\hbox {}. }}{8}}
\newlabel{fig:miss_rate_curve}{{10}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of how a locally-connected graph in 2 dimensions is mapped to a dag via a second-order Hilbert priority function. Each vertex is mapped to its closest grid point in the discretized Hilbert curve. Among vertices mapping to the same Hilbert grid point, ties are broken randomly.}}{8}}
\newlabel{fig:hilbert_priority}{{12}{8}}
\citation{TirthapuraSeAl06}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Throughput vs. number of workers of the \textnormal  {\scshape  BSP} scheduling algorithm, described in Figure\nobreakspace  {}13\hbox {}, for the test graph with 50M vertices described in Section\nobreakspace  {}3.1\hbox {}. The squares correspond to the test graph with randomly ordered vertices and the circles correspond to the same graph reordered according to the Hilbert priority function. }}{9}}
\newlabel{fig:scalability_bsp}{{11}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The Bulk-synchronous Parallel (BSP) scheduling algorithm is the best-case scheduling algorithm in that all vertices are eligible at the beginning and thus \textnormal  {\scshape  BSP} incurs the minimum possible scheduling overhead.}}{9}}
\newlabel{fig:bsp_code}{{13}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Laika}{10}}
\newlabel{sec:laika}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Diagram describing the scheduling algorithm \textnormal  {\scshape  Laika} detailed in Figure\nobreakspace  {}15\hbox {}. The diagram consists of $2^b$-vertex 'chunks', where $b$ is a configuration parameter, each of which is processed serially. Due to the locality resulting from the Hilbert priority function, the neighbors of vertices labeled $u$ and $v$, respectively, predominantly lie in the shaded regions surrounding them. Chunks are processed serially, so it is not necessary to keep track of predecessors and successors that lie within a single chunk. As a result, vertex $u$ merely executes its update function and incurs no overhead for updating the counters of its neighbors, as with \textnormal  {\scshape  JonesPlassmann}. The vertex $v$ illustrates a common phenomenon, where vertices near the beginning of a chunk (i.e. chunk 3) have successors toward the end of the previous chunk (i.e. chunk 2), a \emph  {backward dependency}.}}{10}}
\newlabel{fig:laika_diagram}{{14}{10}}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 109}}{11}}
\newlabel{li:jp_preds}{{19}{11}}
\newlabel{li:jp_succs}{{20}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The Laika scheduling algorithm. The main \textbf  {while} loop spins making progress on all chunks in parallel until all chunks have been completely processed. For each iteration, each chunk is processed serially until it is either done or encounters a vertex $v$ which is not yet eligible to be processed (i.e. $\ensuremath  {v.\ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax \ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax counter}}}}} > 0$). The priority function $\rho (v)$ returns the pair $\delimiter "426830A \ensuremath  {v.\ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax \ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax priority}}}}},\ensuremath  {v.\ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax \ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax chunk}}}}} \delimiter "526930B $ which is comparable lexicographically. The user-supplied function \textnormal  {\scshape  Update} is called for each vertex once all of its predecessors have been processed. The vertex state for the $i$th vertex $v_i$, $\ensuremath  {v_i.\ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax \ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax chunk}}}}} \mathrel {\hspace  {1pt}=\hspace  {1pt}}\mathopen {}\left \delimiter "4262304 i/2^b \right \delimiter "5263305 \mathclose {}$ and $\ensuremath  {v_i.\ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax \ensuremath  {\mathit  {\mathcode `\discretionary {-}{}{}=`\discretionary {-}{}{}\relax priority}}}}} \mathrel {\hspace  {1pt}=\hspace  {1pt}}i \penalty \z@ \mkern 12mu({\rm  mod}\tmspace  +\thinmuskip {.1667em}\tmspace  +\thinmuskip {.1667em}2^b)$ is defined at graph creation.}}{11}}
\newlabel{fig:laika_code}{{15}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Multi-core Experimental Results}{11}}
\newlabel{sec:empirical}{{3.1}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Throughput vs. number of workers on the test graph with 50M vertices described in Section\nobreakspace  {}3.1\hbox {} for three scheduling algorithms: \textnormal  {\scshape  BSP} (i.e. blue), \textnormal  {\scshape  Laika} (i.e. orange) with $b=16$ (i.e. 65,536 vertices per chunk), and \textnormal  {\scshape  JonesPlassmann} (i.e. green). }}{12}}
\newlabel{fig:scalability_original}{{16}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Effects of $k^\textrm  {th}$-order Hilbert Reordering}{12}}
\newlabel{sec:hilbert_bits}{{3.2}{12}}
\citation{TirthapuraSeAl06}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Throughput vs. number of workers on the test graph with 50M vertices described in Section\nobreakspace  {}3.1\hbox {} reordered according to the Hilbert priority function for three scheduling algorithms: \textnormal  {\scshape  BSP} (i.e. blue squares), \textnormal  {\scshape  Laika} (i.e. orange squares), and \textnormal  {\scshape  JonesPlassmann} (i.e. green squares). }}{13}}
\newlabel{fig:scalability_reordered}{{17}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Laika on Distributed Memory}{13}}
\newlabel{sec:partitions}{{4}{13}}
\citation{MPI94}
\citation{MichaelSc96}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Throughput vs. $k$ on the test graph with 50M vertices described in Section\nobreakspace  {}3.1\hbox {} for three single-threaded scheduling algorithms: \textnormal  {\scshape  BSP} (i.e. blue), \textnormal  {\scshape  Laika} (i.e. orange), and \textnormal  {\scshape  JonesPlassmann} (i.e. green). The value $k$ is an input parameter to the Hilbert priority function, which discretizes the unit cube into a $2^k$ x $2^k$ x $2^k$ lattice where vertices lying within the same lattice block are ordered randomly. The special case of $k=0$ (i.e. squares) indicates that the input graph consists of randomly ordered vertices. }}{14}}
\newlabel{fig:scalability_hilbert_bits_serial}{{18}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Results}{14}}
\newlabel{sec:mpi_empirical}{{4.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Same as Figure\nobreakspace  {}18\hbox {} except that the scheduling algorithms are executed with 12 workers.}}{15}}
\newlabel{fig:scalability_hilbert_bits_parallel}{{19}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Examples of how contiguous subintervals yield compact spaces in 2-dimensional space.}}{15}}
\newlabel{fig:hilbert_compact}{{20}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Results of distributed memory experiment executing \textnormal  {\scshape  Laika} extended using MPI on up to 8 individual 12-core Intel Xeons.}}{15}}
\newlabel{fig:mpi_results}{{22}{15}}
\bibstyle{acm}
\bibdata{allpapers}
\bibcite{MPI94}{1}
\bibcite{NYT57}{2}
\bibcite{CuttingCa05}{3}
\bibcite{DeanGh08}{4}
\bibcite{GotsmanLi96}{5}
\bibcite{HarlacherKlRo12}{6}
\bibcite{HasenplaughKaLe14}{7}
\bibcite{Hilbert70}{8}
\bibcite{JonesPl93}{9}
\bibcite{KalerHaSc14}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The construction of the Hilbert curve makes it clear that contiguous subintervals of the curve yields compact volumes in $N$-dimensional space: the curve always makes 90 degree turns in an $N$-dimensional construction, thus every pair of adjacent volumes in the Hilbert curve share a face.}}{16}}
\newlabel{fig:hilbert_construction}{{21}{16}}
\bibcite{KyrolaBlGu12}{11}
\bibcite{LeungPhJo02}{12}
\bibcite{LowBiGo12}{13}
\bibcite{MichaelSc96}{14}
\bibcite{MoonJaFa96}{15}
\bibcite{PilkingtonBa96}{16}
\bibcite{SinghHoHe93}{17}
\bibcite{TirthapuraSeAl06}{18}
\bibcite{WarrenSa93}{19}
