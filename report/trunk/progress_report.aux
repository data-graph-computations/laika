\relax 
\@writefile{toc}{\contentsline {title}{Bigger and Faster Data-graph Computations for Physical Simulations}{1}}
\@writefile{toc}{\authcount {2}}
\@writefile{toc}{\contentsline {author}{Predrag Gruevski, William Hasenplaugh, and James J. Thomas}{1}}
\newlabel{abstract}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\citation{CuttingCa05,DeanGh08}
\citation{LowBiGo12}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A mesh graph where lines correspond to edges and intersections of lines correspond to vertices.}}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mesh}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Data-graph Computations}{2}}
\citation{KalerHaSc14}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example of how a graph can be partitioned into independent sets of vertices denoted by color, each set of which is able to be executed simultaneously without causing data races. Iterating through the colors serially and executing the corresponding independent sets in parallel is a technique called \emph  {chromatic scheduling}.}}{3}}
\newlabel{fig:chromatic_scheduling}{{2}{3}}
\citation{JonesPl93}
\citation{HasenplaughKaLe14}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Graphs are stored in memory on a single cache-coherent multi-core in a sparse-matrix format. The vertex array contains vertex data and an index into an edge array, which contains vertex IDs of the associated neighbors.}}{4}}
\newlabel{fig:layout}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An alternative to chromatic scheduling, which yields a deterministic, data race-free output, is dag scheduling. A priority funcrtion $\rho : V \rightarrow \mathbb  {R}$ is used to create a partial order on the vertices, orienting an edge from low to high priority results in a dag. The vertices are processed in dag order: a vertex is not processed until all of its predecessors have been processed.}}{4}}
\newlabel{fig:dag_scheduling}{{4}{4}}
\citation{Hilbert70}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Simit}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Graphs generated by the language Simit feature hyperedges, an example of which is in blue on the left. Hyperedges are represented by different \emph  {types} of vertices in the resulting data-graph computation. The square vertices in the figure represent hyperedges and have associated per-hyperedge data.}}{5}}
\newlabel{fig:hyperedge}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The Hilbert Space-filling Curve}{5}}
\citation{Hilbert70}
\citation{Hilbert70}
\citation{GotsmanLi96}
\citation{MoonJaFa96,TirthapuraSeAl06}
\citation{LowBiGo12}
\citation{KyrolaBlGu12}
\citation{NYT57}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Graphs generated by the language Simit have tetrahedrons, as depicted on the left above. A tetrahedron is composed of four hyperedges (or \emph  {faces}), an example of which is in blue on the left. Tetrahedra are represented by different \emph  {types} of vertices in the resulting data-graph computation. The diamong vertex on the right represents a tetrahedron and is connected to its four constituent hyperedges.}}{6}}
\newlabel{fig:tetrahedron}{{6}{6}}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 286}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Three recursion levels of a 2D Hilbert space-filling curve\nobreakspace  {}\cite  {Hilbert70}. The red curve is the first recursion level and illustrates the basic inverted 'U' shape. The blue curve shows how each quadrant is partitioned into four independent first-level Hilbert curves (up to rotations) of half the size in each dimension. The black curve illustrates the third recursion level.}}{6}}
\newlabel{fig:2d_hilbert}{{7}{6}}
\citation{SinghHoHe93,WarrenSa93,PilkingtonBa96}
\citation{MoonJaFa96}
\citation{SinghHoHe93,WarrenSa93}
\citation{MoonJaFa96}
\citation{LeungPhJo02}
\citation{HarlacherKlRo12}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The Russian street dog Laika is one of the first and most famous animals to travel through space.}}{7}}
\newlabel{fig:laika}{{8}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Paper organization}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Fast Execution on Individual Machines}{7}}
\newlabel{sec:multicore}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Space-filling Curves}{7}}
\citation{KalerHaSc14}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Example of how a locally-connected graph in 2 dimensions is mapped to a dag via the Hilbert priority function. Each vertex is mapped to its closest grid point in the discretized Hilbert curve. Among vertices mapping to the same Hilbert grid point, ties are broken randomly.}}{8}}
\newlabel{fig:hilbert_priority}{{9}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Graph Representation in Memory}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Miss rate curve...}}{9}}
\newlabel{fig:miss_rate_curve}{{10}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Scalability.}}{9}}
\newlabel{fig:scalability_bsp}{{11}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Scheduling for Parallel Execution}{9}}
\citation{JonesPl93}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Scalability.}}{10}}
\newlabel{fig:scalability_original}{{12}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Scalability.}}{10}}
\newlabel{fig:scalability_reordered}{{13}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Scalability.}}{11}}
\newlabel{fig:scalability_hilbert_bits_serial}{{14}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Scalability.}}{11}}
\newlabel{fig:scalability_hilbert_bits_parallel}{{15}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Achieving Cache Locality}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Considering Parallelism}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Prefetching}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Multi-core Experimental Results}{13}}
\newlabel{sec:experimental_results}{{3}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Graph Partitioning for Distributed Memory}{13}}
\newlabel{sec:partitions}{{4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Examples of how contiguous subintervals yield compact spaces in 2-dimensional space.}}{14}}
\newlabel{fig:hilbert_compact}{{16}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The construction of the Hilbert curve makes it clear that contiguous subintervals of the curve yields compact volumes in $N$-dimensional space: the curve always makes 90 degree turns in an $N$-dimensional construction, thus every pair of adjacent volumes in the Hilbert curve share a face.}}{14}}
\newlabel{fig:hilbert_construction}{{17}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Dealing with Distributed Execution}{14}}
\bibstyle{acm}
\bibdata{allpapers}
\bibcite{NYT57}{1}
\bibcite{CuttingCa05}{2}
\bibcite{DeanGh08}{3}
\bibcite{GotsmanLi96}{4}
\bibcite{HarlacherKlRo12}{5}
\bibcite{HasenplaughKaLe14}{6}
\bibcite{Hilbert70}{7}
\bibcite{JonesPl93}{8}
\@writefile{toc}{\contentsline {section}{\numberline {6}Distributed Memory Experimental Results}{15}}
\newlabel{sec:scaling_out}{{6}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{15}}
\newlabel{sec:conclusion}{{7}{15}}
\bibcite{KalerHaSc14}{9}
\bibcite{KyrolaBlGu12}{10}
\bibcite{LeungPhJo02}{11}
\bibcite{LowBiGo12}{12}
\bibcite{MoonJaFa96}{13}
\bibcite{PilkingtonBa96}{14}
\bibcite{SinghHoHe93}{15}
\bibcite{TirthapuraSeAl06}{16}
\bibcite{WarrenSa93}{17}
